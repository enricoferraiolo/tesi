\section{Conclusioni}
Le analisi e gli esperimenti che sono stati condotti in questa tesi evidenziano un chiaro segno di vulnerabilit\`a dei \emph{Large Language Model}, i quali soffrono di una serie di attacchi mirati capaci di compromettere la loro affidabilit\`a. Come \`e stato visto questi attacchi spaziano dalla manipolazione dell'input, a quella del dataset di addestramento fino agli attacchi mirati ai filtri del modello linguistico. Il panorama del \emph{fooling} di LLM rimane quindi aperto e in continua evoluzione.\\
La tesi ha inoltre analizzato e sperimentato anche due tipologie di attacchi distinti, dimostrando la gravit\`a delle minacce discusse e mettendo l'accento sulla necessit\`a di strategie di difesa per la prevenzione, o mitigazione, di attacchi simili.

\section{Prospettive Future}
Le debolezze emerse dagli esperimenti e discusse nei capitoli precedenti lasciano aperti diversi casi di studio e analisi poich\'e le vulnerabilit\'a dei LLM rappresentano un rischio concreto e tangibile. In futuro sar\`a essenziale sviluppare modelli non solo pi\`u potenti, ma anche maggiormente resilienti e sicuri. Il lavoro svolto in questa tesi vuole evidenziare le sfide e le opportunit\'a in questo campo, sottolineando che una maggiore ricerca nell'ambito del \emph{fooling} dei modelli \emph{Large Language Model} \`e necessaria per uno sviluppo sostenibile e responsabile.