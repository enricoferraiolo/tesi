Di cosa parlare:
\begin{itemize}
    \item Introduzione agli LLM, fare esempi di state of art LLMs
    \item Introduzione a cosa vuol dire fare fooling di un LLM, esempi
    \item attacchi e difese reattive e proattive
    \item Attacchi
    \begin{itemize}
        % \item blackbox e whitebox
        \item leeching
        \item poisoning
        \begin{itemize}
            \item dataset poisoning
            \item weight poisoning
        \end{itemize}
        \item jailbreaking LLMs
    \end{itemize}
    \item esperimenti, provare varie tipologie di attacchi
    \item Difesa
        \begin{itemize}
            \item jailbreaking
            \begin{itemize}
                \item cap. 5.2 di \cite{kumar2024adversarialattacksdefensesllm}
            \end{itemize}
            \item fine-tuning
        \end{itemize}
    \item riepilogo e valutazioni
        \begin{itemize}
            \item vale la pena attaccare un LLM?
            \item attaccare un LLM richiede tante risorse e tempo, forse troppe
            \item quali risorse bisogna tenere in conto per attaccare un LLM? (dataset size, tempo richiesto per attaccare, efficacia dell'attacco), conviene?
        \end{itemize}
\end{itemize}