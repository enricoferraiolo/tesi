\section*{Abstract}
Negli ultimi anni, i \emph{Large Language Model} (LLM) hanno rappresentato una rivoluzione nell'elaborazione del linguaggio naturale grazie alle loro capacit\`a generative e alla comprensione del contesto. Tuttavia questi modelli sono vulnerabili a diversi tipi di attacchi che possono comprometterne la sicurezza e l'affidabilit\`a.\\
Questo studio vuole formalizzare il concetto degli attacchi ai \emph{Large Language Model} dando una caratterizzazione matematica al problema.\\
Gli esperimenti condotti dimostrano la facilit\`a con cui tali attacchi possono manipolare il comportamento del modello, evidenziando rischi per l'integrit\`a e la sicurezza per i proprietari e gli utilizzatori.\\
Questa tesi, inoltre, affronta ed esplora le principali metodologie di attacco ai LLM sperimentando approfonditamente le tecniche di \emph{Prompt Injection} e \emph{Data Poisoning}.