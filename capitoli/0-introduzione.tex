Negli ultimi anni i \emph{Large Language Model} (LLM) sono stati una delle pi\`u grandi innovazioni nel campo del \emph{Natural Language Processing} (NLP). Questi modelli linguistici sono basati su architetture come i \emph{Transformer}. Sono in grado di elaborare e generare testo in linguaggio naturale simulando un dialogo umano. Grazie alle loro forti capacit\`a di analisi di contesti complessi e di risposta coerente alla conversazione i LLM sono stati adottati in numerosi settori e campi, ci\`o per\`o implica anche una maggiore preoccupazione in ambito di sicurezza poich\`e aumenta il rischio di \emph{fooling}.\\

Il concetto di \emph{fooling} fa riferimento alla capacit\`a di indurre un modello a comportarsi in modo inatteso producendo output errati, incoerenti o addirittura pericolosi. Non \`e banale la differenza tra \emph{fooling} sulle \emph{Deep Neural Network} (DNN) e \emph{fooling} sui LLM, infatti il primo si basa principalmente sulla manipolazione  dell'input del modello di classificazione per ottenere una risposta erroneamente classificata, sfruttando le vulnerabilit\`a legate alla rappresentazione delle \emph{feature}. Fare \emph{fooling} su \emph{Large Language Model}, invece, il problema \`e decisamente pi\`u complicato e questo tema verr\`a approfondito nel corso della tesi.\\
La natura del problema appena descritto lascia la strada aperta a numerose forme diverse di attacco, a seconda degli obiettivi e delle modalit\`a di accesso al modello. Questo lavoro tenta di dare una formalizzazione al problema cercando di generalizzare un attacco a un modello linguistico.\\

Per comprendere a pieno l'impatto che hanno sulla realt\`a gli attacchi si \`e rivelato molto utile condurre esperimenti pratici. Questi sono stati progetti e realizzati per verificare le tecniche di \emph{fooling} descritte in questo lavoro.\\

\`E quindi chiaro che questa tesi non cerca solo di analizzare la teoria alla base degli attacchi e darne formalizzazione matematica, ma anche di tradurre tali concetti in esperimenti pratici cercando di esplorare le implicazioni di sicurezza per i \emph{Large Language Model}.